---
title: "Classification of the Alzheimer's Disease Using a SVM Model"
subtitle: "Kaggle Competition Report"
author: 
  - Rahma Binth Mohammad
thanks: "Code and data are available at: [https://github.com/rahmabinth/alzheimers](https://github.com/rahmabinth/alzheimers)."
abstract: "This report is based on project that used supervised machine learning techniques to build a model that classifies those diagnosed with Alzheimers. After preprocessing and feature selection, we evaluated several algorithms and selected the best-performing model based on predictive accuracy. Our final model achieved a Kaggle score of _____, ranking ____ among the other models."
format: pdf
number-sections: true
toc: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(palmerpenguins)
library(patchwork)

# Read in the data
setwd("/Users/Rahma/alzheimers")
train_data <- read.csv("data/01-raw_data/train.csv", stringsAsFactors = FALSE)

test_data <- read.csv("data/01-raw_data/test.csv", stringsAsFactors = FALSE)

# Remove the 'DoctorInCharge' column, irrelevant to our problem
train_data <- select(train_data, -DoctorInCharge)
test_data <- select(test_data, -DoctorInCharge)
```

# Introduction

There were over 55 million people worldwide living with dementia in 2020 and by 2050, it is projected to affect 139 million people (cite Alzheimer's International). Due to the fact that this is a huge amount of the population, it is important to identify the characteristics and lifestyle choices of the people it affects in order to find a pattern or a prevention method. Finding a pattern can help with early detection, which would allow for better prevention. Thus, this paper focuses on predicting Alzheimer's Disease in patients using a dataset from Kaggle that contains data collected from patients. We treated the diagnosis as a binary outcome (1 = Yes, 0 = No) and aimed to use machined learning algorithms to find the best model for performance. 

Our final and best performing model achieved a score of _____ and ranked ______ on the leaderboard. The model performed as follows. Evaluation metrics -----> Strongly in AUC or F1 score (find out). 

Our model matters because it allows for early detection of Alzheimers and demonstrates the potential of machine learning in highlighting key predictive factors in patient data. 

The remainder of this paper is structured as follows. @sec-data....

# Data {#sec-data}

Kaggle Username: rahmabm

Final Kaggle Ranking:

Final Private Score: 

## Overview {#sec-overview}

We conducted our analysis using the statistical programming language R [@citeR]. Our data was pulled from the Kaggle Competition called Classification of the Alzheimer's Disease dataset [cite competition]. The dataset provides patient data for _____ patients. Each row contains a different individuals patient data with ___ different variables found at [cite]]. It contains demographic details, lifestyle factors, medical history, cognitive assessments, and symptoms for each patient. 

## Measurement {#sec-measurement}

While the dataset is based on real-world patient assessments and health records, certain variables are expressed as measurable features with the use of standardizes clinic tests. This includes several of the cognitive and function assessments in the reported data such as Mini-Mental State Examination Score, Functional Assessment Score, and Behavioural Problems. Others have been self-reported as binary variables, such as the symptoms. Variables such as Confusion, measured the presence of confusion, along with variables like Disorientation and Forgetfulness. 

Some paragraphs about how we go from a phenomena in the world to an entry in the dataset.


```{r}
#| label: fig-cog
#| fig-cap: Diagnosis vs. Cognitive Issues
#| fig-align: "center"
#| echo: false
library(dplyr)
library(ggplot2)

## Add a column that indicates if the person has at least one cognitive issue
train_data <- train_data %>% 
  mutate(HasCognitiveIssues = ifelse(MMSE < 10 | 
                                       FunctionalAssessment < 5 | 
                                       MemoryComplaints == 1 | 
                                       BehavioralProblems == 1 | 
                                       ADL < 5, 1, 0))


# Count the number of people in each group (HasCognitiveIssues vs Diagnosis)
summary_cognitive <- train_data %>%
  group_by(HasCognitiveIssues, Diagnosis) %>%
  summarise(Count = n(), .groups = "drop")

# Plot HasCognitiveIssues vs Diagnosis
ggplot(summary_cognitive, aes(x = as.factor(HasCognitiveIssues), 
                              y = Count, fill = as.factor(Diagnosis))) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_x_discrete(labels = c("0" = "None", "1" = "One or More")) +
  scale_fill_manual(name = "Diagnosis",
    values = c("0" = "deepskyblue4", "1" = "darkred"),
    labels = c("No", "Yes")) +
  labs(x = "Cognitive Issues",
       y = "Number of People") +
  theme(axis.title.x = element_text(size = 10),
    axis.title.y = element_text(size = 10)) +
  theme(legend.position = "right")   

```

## Data Preprocessing and Feature Engineering {#sec-preprocessing}

We started by inspecting the data for missing values and irrelevant variables. We found variables such as the “DoctorInCharge”, and “PatientID” to be irrelevant to our problem and removed them. This also allowed for randomness in our data. We looked for patterns within the data but segregating the data into separate categories. We identified those with anytime type of cognitive issues and looked for a pattern within those diagnosed and undiagnosed with Alzeheimers. These issues were found through cognitive and functional assessments as mentioned earlier. They include ______. We found that 88% of the patients had one or more cognitive issue and approximately 40% of them were diagnosed with the diseases. The most vital/significant find was that only 1% of those undiagnosed with the disease had one or more cognitive issues and approximately 40% of those with cognitive issues were diagnosed with the disease. This indicated a very high chance/correlation that if a patient had a cognitive issue, they were more likely to have the disease. 

```{r}
#| label: fig-medsym
#| fig-cap: Diagnosis vs. Medical History and Diagnosis vs. Symptoms
#| fig-align: "center"
#| echo: false

library(dplyr)
library(ggplot2)

## Add a column that indicates if the person has at least one medical condition 
train_data <- train_data %>% 
  mutate(HasMedicalCondition = ifelse(FamilyHistoryAlzheimers == 1 | 
                                        CardiovascularDisease == 1 | 
                                        Diabetes == 1 | 
                                        Depression == 1 | 
                                        HeadInjury == 1 | 
                                        Hypertension == 1, 1, 0))

# Count the number of people in each group (HasMedicalCondition vs Diagnosis)
summary_med <- train_data %>%
  group_by(HasMedicalCondition, Diagnosis) %>%
  summarise(Count = n(), .groups = "drop")

# Plot HasMedicalCondition vs Diagnosis
plot_med <- ggplot(summary_med, aes(x = as.factor(HasMedicalCondition), 
                         y = Count, fill = as.factor(Diagnosis))) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_x_discrete(labels = c("0" = "None", "1" = "One or More")) +
  scale_fill_manual(name = "Diagnosis",
                    values = c("0" = "deepskyblue4", "1" = "darkred"),
                    labels = c("No", "Yes")) +
  labs(x = "Medical Conditions",
       y = "Number of People") +
  theme(axis.title.x = element_text(size = 10),
    axis.title.y = element_text(size = 10)) +
  theme(legend.position = "top")   

# Add a column that indicates if the person has at least one symptom
train_data <- train_data %>% 
  mutate(HasSymptoms = ifelse(Confusion == 1 | 
                                Disorientation == 1 | 
                                PersonalityChanges == 1 | 
                                DifficultyCompletingTasks == 1 | 
                                Forgetfulness == 1, 1, 0))

# Count the number of people in each group (HasSymptoms vs Diagnosis)
summary_symptoms <- train_data %>%
  group_by(HasSymptoms, Diagnosis) %>%
  summarise(Count = n(), .groups = "drop")

# Plot HasSymptoms vs Diagnosis
plot_sym <- ggplot(summary_symptoms, aes(x = as.factor(HasSymptoms), 
                             y = Count, fill = as.factor(Diagnosis))) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_x_discrete(labels = c("0" = "None", "1" = "One or More")) +
  scale_fill_manual(name = "Diagnosis",
                    values = c("0" = "deepskyblue4", "1" = "darkred"),
                    labels = c("No", "Yes")) +
  labs(x = "Symptoms",
       y = "Number of People") +
  theme(axis.title.x = element_text(size = 10),
    axis.title.y = element_text(size = 10)) +
  theme(legend.position = "top")   

plot_med_sym <- plot_med + plot_sym +
  plot_layout(ncol = 2) 

print(plot_med_sym)
```

We did the same with medical history. If the patient had one or more of the the following, we grouped them, a family history of the disease, cardiovascular disease, diabetes, depression, history of head injury, hypertension. We found approximately 66% of the patients had one or more medical condition and 35% of them were diagnosed. It was also shown that 35% of those diagnosed didn’t have one or more of these medical condition. It was less conclusive then those with cognitive issues but we found it relevant to test out models with this feature later on, since a significant number of those with the condition were diagnosed. We saw a similar pattern with patient symptoms, where we grouped together those with one or more of the symptoms, “Confusion”, “Disorientation, “PersonalityChanges”, “DifficultCompletingTasks”, and “Forgetful”. See @fig-medsym.

## Feature Selection {#sec-select}

In addition, we looked at how each variable/factor played into the diagnosis. @Sec-appendix.2 provides the code to replicate this by changing the name for the variable. Our analysis revealed that certain features had the most impact. For example, by examining all the variables, we saw 61% of those diagnosed were Caucasians, and 61% of those diagnosed either didn’t attend high school or only attended high school and didn’t hold a Bachelor’s Degree or Higher Education. However, gender didn’t seem to play a role as the number of men and women diagnosed were approximately the same. 

# Model
Our initial goal for our modelling strategy was twofold, balancing interpretability and predictive accuracy. With this in mind, we used the three factors mentioned in @sec-preprocessing, cognitive issues, medical conditions, and symptoms, we tried to create a logistic regression model and used cross validation method to train and test data but failed to get higher than 62% predictability rate. Hence, we prioritized predictive performance by choosing to do a Support Vector Machine model, which offered limited interpretability. The SVM performed significantly better than our previous tested models. However, interpretability decreased. @sec-discussion identifies this tradeoff.  

## Model set-up

We define the outcome variable $y_i \in \{0, 1\}$ as the diagnosis of Alzheimer's Disease, where $y_i = 1$ indicates a positive diagnosis and $y_i = 0$ indicates a negative diagnosis. To predict this outcome, we trained a Support Vector Machine (SVM) classifier using the following selected predictor variables whether the feature for individual $i$ is denoted by $x_i = (x_1, x_2,...,x_p)$.

$x_1$: Ethnicity, $x_2$: Gender, $x_3$: MMSE score (Mini-Mental State Examination score), $x_4$: MemoryComplaints, $x_5$: ADL, $x_6$: FunctionalAssessment, $x_7$: BehavioralProblems, $x_8$: HeadInjury, $x_9$: Forgetfulness

The model estimates a function $f(\mathbf{x}_i)$ such that:

$$
\hat{y}_i = \begin{cases}
1 & \text{if } f(\mathbf{x}_i) \geq 0 \\
0 & \text{if } f(\mathbf{x}_i) < 0
\end{cases}
$$


We run the model in R [@citeR] using the `e1071` package [@e1071]. In our SVM model, $f(\mathbf{x})$ is the decision function — a number that tells how far a point is from the decision boundary (i.e., the separating surface between classes).

If $f(\mathbf{x}_i) \geq 0$, the model predicts class 1 (positive diagnosis).  
If $f(\mathbf{x}_i) < 0$, it predicts class 0 (negative diagnosis).



### Model justification


## Evaluation Metrics and Model Performance {#sec-eval}

Before submitting on Kaggle, we worked with the “train_csv” file and split it into two, 70% of the data was used for training and 30% used for validation. Our validation set proved useful when testing the models. Hyperparameter tuning via 10-fold cross validation helped identify the optimal parameters, cost = 10 and gamma = 0.1. These parameters provided a cross-validated error rate of 0.0997, which shows that the model misclassifies about 10.26% of observations and will do so on new and similar data. 

Our Support Vector Machine (SVM) classifier with these parameters achieved an accuracy of 89% and sensitivity of 90.1% (accurately identifying the undiagnosed patients) and specificity of 87% (accurately identifying the diagnosed patients) on the validation set. The ROC curve in @fig-roc shows this relationship and plots how well our model separates the two classes. The area under the curve (AUC) of 0.941 shows that our model has excellent ability to distinguish between Alzheimer's-positive and Alzheimer's-negative patients.

Note we chose a radial kernel rather than a linear one as it provided about 9% less accuracy.


```{r}
#| label: fig-roc
#| fig-cap: ROC curve
#| echo: false
#| warning: false
#| message: false

library(e1071)
library(caret)
library(pROC)
setwd("/Users/Rahma/alzheimers")
set.seed(123)
#Split the data with similar proportions of diagnosed and undiagnoses
index_70 <- createDataPartition(train_data$Diagnosis, p = 0.7, list = FALSE)
train_70 <- train_data[index_70,]
validation_set <- train_data[-index_70,]

train_70$Diagnosis <- as.factor(train_70$Diagnosis)

# Convert 0/1 columns to factors
binary_to_factor <- function(df) {
  binary_cols <- sapply(df, function(col) all(col %in% c(0, 1)))
  df[binary_cols] <- lapply(df[binary_cols], factor)
  return(df)
}

train_70 <- binary_to_factor(train_70)
validation_set <- binary_to_factor(validation_set)

# Remove the 'PAtientID' column, irrelevant to our problem
train_70 <- select(train_70, -PatientID)
validation_set <- select(validation_set, -PatientID)

# Select features to be used in SVM based on the trends found previously
selected_features_70 <- c("Ethnicity", 
                       "Gender",
                       "MMSE",
                       "MemoryComplaints", 
                       "ADL", 
                       "FunctionalAssessment", 
                       "BehavioralProblems",
                       "HeadInjury", 
                       "Forgetfulness")

# Subset to selected features + target
train_subset <- train_70[, c(selected_features_70, "Diagnosis")]
test_subset <- validation_set[, c(selected_features_70, "Diagnosis")]

set.seed(123)
svm_model_test <- svm(Diagnosis ~ ., 
                 data = train_subset, 
                 kernel = "radial",  # or "radial"
                 cost = 10,  
                 gamma = 0.1, # you can tune this
                 scale = TRUE,
                 class.weights = c("0" = 1, "1" = 1.3))       

predictions <- predict(svm_model_test, newdata = test_subset[, selected_features_70])

# Get decision values (probabilities or distance from hyperplane)
svm_model_prob <- predict(svm_model_test, newdata = test_subset[, selected_features_70], decision.values = TRUE)

# Extract decision values (needed for ROC)
decision_values <- attributes(svm_model_prob)$decision.values

# Create ROC curve
roc_obj <- roc(test_subset$Diagnosis, as.numeric(decision_values))

#Plot ROC curve with value
plot(roc_obj, col = "darkred", main = paste("AUC:", round(auc(roc_obj), 3)))
```


# Results

As mentioned in @sec-select, all the variables were examined to see a ___. However, when we simply inputted those features into the SVM, it only gave us an accuracy of 86%. To improve our model and better understand each feature’s importance, we conducted a leave-one-feature-out sensitivity check, essentially removing one feature at a time, retraining the model, and checking how the accuracy changes. Using this, we found certain features to be extremely important to the model’s predictive ability. For example, MMSE and FunctionalAssessment were very important. This check also allowed us identify which features to retain and exclude. The final set of features used is listen in @sec-model-set and reflects the most significant contributors to the Alzheimer’s disease. @sec-discussion will examine the relevance of these features. 


# Discussion

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this. 

## Second discussion point

Please don't use these as sub-heading labels - change them to be what your point actually is.

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {-}


# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In we implement a posterior predictive check. This shows...

In we compare the posterior with the prior. This shows... 

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

#pp_check(first_model) +
  #theme_classic() +
  #theme(legend.position = "bottom")

#posterior_vs_prior(first_model) +
  #theme_minimal() +
  #scale_color_brewer(palette = "Set1") +
  #theme(legend.position = "bottom") +
  #coord_flip()
```

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

#plot(first_model, "trace")

#plot(first_model, "rhat")
```



\newpage


# References


